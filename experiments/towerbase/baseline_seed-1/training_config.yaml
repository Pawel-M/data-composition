results_dir: .
results_suffix: null
tokenizer_name: 'towerbase_7B'
model_path: "Unbabel/TowerBase-7B-v0.1"
tokenizer_path: "Unbabel/TowerBase-7B-v0.1"
max_length: 2048
lang_pairs:
  - "en-de"
  - "de-en"
  - "en-es"
  - "es-en"
  - "en-fr"
  - "fr-en"
#  - "en-pl"
#  - "pl-en"
  - "en-ru"
  - "ru-en"
src_ctx_size: 3
tgt_ctx_size: 3
seed: 1

#datasets:
#  -
#    dataset_name: iwslt2017
#    dataset_splits: [train]
#    dataset_path: ~/fine-tuning-ctx-mt/data/hf_iwslt17
#    dataset_annotation_path: ~/VOXReality/data/iwslt2017Yin
#    sample_ctx_size: true
#    lang_pairs:
#      - "en-de"
#    filter_out: true
#    filtered_phenomena:
#      - "gender"
#      - "auxiliary"
#      - "formality"
#      - "inflection"
#      - "animacy"
#    limit_size: 10_000

datasets:
  - # OS random
    dataset_name: opensubtitles
    raw_dataset_path: $PROJECT_HOME/Datasets/ctxpro/data/opensubs/
    dataset_path: $PROJECT_HOME/fine-tuning-ctx-mt/data/hf_opensubtitles
    dataset_annotation_path: $PROJECT_HOME/Datasets/ctxpro/release
    train_size: 50_000
    split_seed: 1
    sample_ctx_size: true
    lang_pairs:
      - "en-de"
      - "de-en"
      - "en-es"
      - "es-en"
      - "en-fr"
      - "fr-en"
#      - "en-pl"
#      - "pl-en"
      - "en-ru"
      - "ru-en"
#  - # OS rich
#    dataset_name: ctxpro
#    raw_dataset_path: $PROJECT_HOME/Datasets/ctxpro/data/opensubs/
#    base_dataset_path: $PROJECT_HOME/fine-tuning-ctx-mt/data/hf_opensubtitles
#    dataset_annotation_path: $PROJECT_HOME/Datasets/ctxpro/release
#    processed_dataset_path: $PROJECT_HOME/fine-tuning-ctx-mt/data/hf_opensubtitles_ctxpro
#    sample_ctx_size: true
#    limit_size: 12_000
#    lang_pairs:
#      - "en-de"
#    filtered_phenomena:
##      - "gender"
#    #  - "auxiliary"
#      - "formality"
#    #  - "inflection"
#    #  - "animacy"

training_arguments:
  optim: adafactor
  learning_rate: 0.00001
  lr_scheduler_type: linear
  warmup_ratio: 0.1
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 4
  weight_decay: 0.01
  save_total_limit: 3
  num_train_epochs: 3
  fp16: false
  bf16: true

lora_arguments:
  lora_alpha: 32
  lora_dropout: 0.0
  r: 16
  bias: "none"
  target_modules: ["q_proj", "k_attn", "v_attn", "o_proj", "gate_proj", "up_proj", "down_proj"]

#'4bits', '8bits', '16bits'
quantization: '16bits'